{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "_dSLU6BvOeeK",
        "outputId": "60c753fe-49c0-4b02-eb62-4c1911242ad8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       id  \\\n",
              "0  716549   \n",
              "1  715742   \n",
              "2  727333   \n",
              "3  606874   \n",
              "4  505318   \n",
              "\n",
              "   Q1: From a scale 1 to 5, how complex is it to make this food? (Where 1 is the most simple, and 5 is the most complex)  \\\n",
              "0                                                  3                                                                       \n",
              "1                                                  4                                                                       \n",
              "2                                                  3                                                                       \n",
              "3                                                  4                                                                       \n",
              "4                                                  2                                                                       \n",
              "\n",
              "  Q2: How many ingredients would you expect this food item to contain?  \\\n",
              "0                                                  6                     \n",
              "1                                        bread, meet                     \n",
              "2                                                  5                     \n",
              "3                                                6-7                     \n",
              "4                                          3 or more                     \n",
              "\n",
              "  Q3: In what setting would you expect this food to be served? Please check all that apply  \\\n",
              "0         Week day lunch,At a party,Late night snack                                         \n",
              "1         Week day lunch,At a party,Late night snack                                         \n",
              "2  Week day lunch,Week day dinner,Weekend lunch,W...                                         \n",
              "3  Week day lunch,Week day dinner,Weekend lunch,W...                                         \n",
              "4  Week day lunch,Week day dinner,Weekend lunch,W...                                         \n",
              "\n",
              "  Q4: How much would you expect to pay for one serving of this food item?  \\\n",
              "0                                                  5                        \n",
              "1                               5$ for a large piece                        \n",
              "2                                           10dollar                        \n",
              "3                                                $3                         \n",
              "4                                                $5                         \n",
              "\n",
              "  Q5: What movie do you think of when thinking of this food item?  \\\n",
              "0                  Cloudy with a Chance of Meatballs                \n",
              "1              All sort of american young boy movies                \n",
              "2                                       action movie                \n",
              "3                                          Mamma Mia                \n",
              "4                  Cloudy with a chance of meatballs                \n",
              "\n",
              "  Q6: What drink would you pair with this food item?  \\\n",
              "0                                              CokeÂ    \n",
              "1                                               Coke   \n",
              "2                                               cola   \n",
              "3                                               Soda   \n",
              "4                                               Soda   \n",
              "\n",
              "  Q7: When you think about this food item, who does it remind you of?  \\\n",
              "0                                            Friends                    \n",
              "1                         Friends,Teachers,Strangers                    \n",
              "2                                            Friends                    \n",
              "3                          Siblings,Friends,Teachers                    \n",
              "4                                   Siblings,Friends                    \n",
              "\n",
              "  Q8: How much hot sauce would you add to this food item?  Label  \n",
              "0                                    A little (mild)       Pizza  \n",
              "1                                                NaN       Pizza  \n",
              "2                         A moderate amount (medium)       Pizza  \n",
              "3  I will have some of this food item with my hot...       Pizza  \n",
              "4                                    A little (mild)       Pizza  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f99cc82e-fc3f-4644-99cc-c0854fcf3938\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Q1: From a scale 1 to 5, how complex is it to make this food? (Where 1 is the most simple, and 5 is the most complex)</th>\n",
              "      <th>Q2: How many ingredients would you expect this food item to contain?</th>\n",
              "      <th>Q3: In what setting would you expect this food to be served? Please check all that apply</th>\n",
              "      <th>Q4: How much would you expect to pay for one serving of this food item?</th>\n",
              "      <th>Q5: What movie do you think of when thinking of this food item?</th>\n",
              "      <th>Q6: What drink would you pair with this food item?</th>\n",
              "      <th>Q7: When you think about this food item, who does it remind you of?</th>\n",
              "      <th>Q8: How much hot sauce would you add to this food item?</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>716549</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>Week day lunch,At a party,Late night snack</td>\n",
              "      <td>5</td>\n",
              "      <td>Cloudy with a Chance of Meatballs</td>\n",
              "      <td>Coke</td>\n",
              "      <td>Friends</td>\n",
              "      <td>A little (mild)</td>\n",
              "      <td>Pizza</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>715742</td>\n",
              "      <td>4</td>\n",
              "      <td>bread, meet</td>\n",
              "      <td>Week day lunch,At a party,Late night snack</td>\n",
              "      <td>5$ for a large piece</td>\n",
              "      <td>All sort of american young boy movies</td>\n",
              "      <td>Coke</td>\n",
              "      <td>Friends,Teachers,Strangers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pizza</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>727333</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>Week day lunch,Week day dinner,Weekend lunch,W...</td>\n",
              "      <td>10dollar</td>\n",
              "      <td>action movie</td>\n",
              "      <td>cola</td>\n",
              "      <td>Friends</td>\n",
              "      <td>A moderate amount (medium)</td>\n",
              "      <td>Pizza</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>606874</td>\n",
              "      <td>4</td>\n",
              "      <td>6-7</td>\n",
              "      <td>Week day lunch,Week day dinner,Weekend lunch,W...</td>\n",
              "      <td>$3</td>\n",
              "      <td>Mamma Mia</td>\n",
              "      <td>Soda</td>\n",
              "      <td>Siblings,Friends,Teachers</td>\n",
              "      <td>I will have some of this food item with my hot...</td>\n",
              "      <td>Pizza</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>505318</td>\n",
              "      <td>2</td>\n",
              "      <td>3 or more</td>\n",
              "      <td>Week day lunch,Week day dinner,Weekend lunch,W...</td>\n",
              "      <td>$5</td>\n",
              "      <td>Cloudy with a chance of meatballs</td>\n",
              "      <td>Soda</td>\n",
              "      <td>Siblings,Friends</td>\n",
              "      <td>A little (mild)</td>\n",
              "      <td>Pizza</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f99cc82e-fc3f-4644-99cc-c0854fcf3938')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f99cc82e-fc3f-4644-99cc-c0854fcf3938 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f99cc82e-fc3f-4644-99cc-c0854fcf3938');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f9d114af-411d-4418-ab6f-86a973153d4b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f9d114af-411d-4418-ab6f-86a973153d4b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f9d114af-411d-4418-ab6f-86a973153d4b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1644,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 125479,\n        \"min\": 5978,\n        \"max\": 854745,\n        \"num_unique_values\": 548,\n        \"samples\": [\n          523710,\n          631507,\n          629285\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Q1: From a scale 1 to 5, how complex is it to make this food? (Where 1 is the most simple, and 5 is the most complex)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Q2: How many ingredients would you expect this food item to contain?\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 493,\n        \"samples\": [\n          \"Rice, fish, wasabi, seaweed, salt, soy sauce.\\n\\natleast 6\",\n          \"Around 6-8, not including spices/seasoning\",\n          \"10+\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Q3: In what setting would you expect this food to be served? Please check all that apply\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 60,\n        \"samples\": [\n          \"Week day lunch,At a party,Late night snack\",\n          \"Week day dinner,Weekend lunch,Weekend dinner,At a party,Late night snack\",\n          \"Week day lunch,Week day dinner,Weekend lunch,Late night snack\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Q4: How much would you expect to pay for one serving of this food item?\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 586,\n        \"samples\": [\n          \"A box of sushi would be around 8 dollar.\",\n          \"$20 maximum for a large pizza filled with toppings such as Chicken.\\u00a0\",\n          \"10 dollars.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Q5: What movie do you think of when thinking of this food item?\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 769,\n        \"samples\": [\n          \"Pokemon\",\n          \"A Silent Voice (2016)\",\n          \"Avengers (2012)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Q6: What drink would you pair with this food item?\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 428,\n        \"samples\": [\n          \"Ocha\",\n          \"Lemonade\",\n          \"milk\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Q7: When you think about this food item, who does it remind you of?\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"Siblings,Strangers\",\n          \"Siblings,Friends,Strangers\",\n          \"Parents,Friends,Teachers\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Q8: How much hot sauce would you add to this food item?\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"A moderate amount (medium)\",\n          \"A lot (hot)\",\n          \"A little (mild)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Pizza\",\n          \"Shawarma\",\n          \"Sushi\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = \"/content/drive/My Drive/cleaned_data_combined_modified.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Processing**"
      ],
      "metadata": {
        "id": "Fi4rz1LaOj2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "# LOAD DATA\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/cleaned_data_combined_modified.csv\")\n",
        "\n",
        "# NUMERIC CLEANING\n",
        "def extract_numeric(value):\n",
        "    if pd.isnull(value):\n",
        "        return None\n",
        "    numbers = re.findall(r\"\\d+\\.?\\d*\", str(value))\n",
        "    return np.mean([float(n) for n in numbers]) if numbers else None\n",
        "\n",
        "numerical_columns = [\n",
        "    \"Q1: From a scale 1 to 5, how complex is it to make this food? (Where 1 is the most simple, and 5 is the most complex)\",\n",
        "    \"Q2: How many ingredients would you expect this food item to contain?\",\n",
        "    \"Q4: How much would you expect to pay for one serving of this food item?\"\n",
        "]\n",
        "\n",
        "# Apply numeric extraction\n",
        "for col in numerical_columns:\n",
        "    df[col] = df[col].apply(extract_numeric)\n",
        "\n",
        "# Fill missing with mean\n",
        "for col in numerical_columns:\n",
        "    mean_val = df[col].mean()\n",
        "    df[col] = df[col].fillna(mean_val)\n",
        "\n",
        "# TEXT CLEANING\n",
        "text_cols = [\n",
        "    \"Q3: In what setting would you expect this food to be served? Please check all that apply\",\n",
        "    \"Q5: What movie do you think of when thinking of this food item?\",\n",
        "    \"Q6: What drink would you pair with this food item?\",\n",
        "    \"Q7: When you think about this food item, who does it remind you of?\"\n",
        "]\n",
        "\n",
        "for col in text_cols:\n",
        "    df[col] = df[col].fillna(\"none\").str.lower().str.strip()\n",
        "\n",
        "# BINARY BAG-OF-WORDS\n",
        "def simple_bow(df, column_name, prefix):\n",
        "    vocab = set()\n",
        "    tokenized = []\n",
        "\n",
        "    # Tokenize and build vocab\n",
        "    for text in df[column_name]:\n",
        "        tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "        tokenized.append(tokens)\n",
        "        vocab.update(tokens)\n",
        "\n",
        "    vocab = sorted(vocab)\n",
        "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "    # Create binary BoW matrix\n",
        "    bow_matrix = np.zeros((len(df), len(vocab)), dtype=int)\n",
        "    for i, tokens in enumerate(tokenized):\n",
        "        for token in tokens:\n",
        "            if token in vocab_index:\n",
        "                bow_matrix[i, vocab_index[token]] = 1\n",
        "\n",
        "    return pd.DataFrame(bow_matrix, columns=[f\"{prefix}_{word}\" for word in vocab])\n",
        "\n",
        "# Apply BoW to each column\n",
        "bow_frames = []\n",
        "for i, col in enumerate(text_cols):\n",
        "    bow_frames.append(simple_bow(df, col, f\"Q{i+3}\"))\n",
        "\n",
        "df = pd.concat([df] + bow_frames, axis=1)\n",
        "df.drop(columns=text_cols, inplace=True)\n",
        "\n",
        "# ONE-HOT ENCODING: Q8\n",
        "hot_sauce_map = {\n",
        "    \"A little (mild)\": \"Mild\",\n",
        "    \"A moderate amount (medium)\": \"Medium\",\n",
        "    \"A lot (hot)\": \"Hot\",\n",
        "    \"I will have some of this food item with my hot sauce\": \"Medium\"\n",
        "}\n",
        "q8_col = \"Q8: How much hot sauce would you add to this food item?\"\n",
        "df[\"Q8_cleaned\"] = df[q8_col].map(hot_sauce_map).fillna(\"None\")\n",
        "\n",
        "# Manually one-hot encode\n",
        "for category in df[\"Q8_cleaned\"].unique():\n",
        "    df[f\"Q8_cleaned_{category}\"] = (df[\"Q8_cleaned\"] == category).astype(int)\n",
        "df.drop(columns=[q8_col, \"Q8_cleaned\"], inplace=True)\n",
        "\n",
        "# CLEAN TYPES\n",
        "df.dropna(inplace=True)\n",
        "for col in df.columns:\n",
        "    if df[col].dtype in [\"float64\", \"bool\"]:\n",
        "        df[col] = df[col].astype(int)\n",
        "\n",
        "# SPLIT FEATURES AND LABEL\n",
        "X = df.drop(columns=[\"Label\"])\n",
        "y = df[\"Label\"]\n",
        "\n",
        "# STRATIFIED SPLIT\n",
        "def stratified_split(X, y, test_size=0.3, random_state=42):\n",
        "    np.random.seed(random_state)\n",
        "    X = X.reset_index(drop=True)\n",
        "    y = y.reset_index(drop=True)\n",
        "    train_idx, test_idx = [], []\n",
        "\n",
        "    for label in y.unique():\n",
        "        idx = y[y == label].index.tolist()\n",
        "        np.random.shuffle(idx)\n",
        "        split = int(len(idx) * (1 - test_size))\n",
        "        train_idx += idx[:split]\n",
        "        test_idx += idx[split:]\n",
        "\n",
        "    return (\n",
        "        X.iloc[train_idx].values.astype(np.float64),\n",
        "        X.iloc[test_idx].values.astype(np.float64),\n",
        "        y.iloc[train_idx].values,\n",
        "        y.iloc[test_idx].values\n",
        "    )\n",
        "\n",
        "X_train, X_test, y_train, y_test = stratified_split(X, y)\n",
        "\n",
        "# Convert X_test (NumPy) back to DataFrame using original column names\n",
        "X_test_df = pd.DataFrame(X_test, columns=X.columns)\n",
        "\n",
        "# Add labels back\n",
        "X_test_df[\"Label\"] = y_test\n",
        "\n",
        "# Save to a test CSV\n",
        "X_test_df.to_csv(\"/content/drive/MyDrive/test_split_only.csv\", index=False)\n",
        "\n",
        "print(\"Shape:\", X.shape, \"| Train:\", X_train.shape, \"| Test:\", X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3afKgPnwOjnJ",
        "outputId": "ca2b73f8-eadf-492b-f445-4573de40501f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (1644, 1367) | Train: (1149, 1367) | Test: (495, 1367)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes**"
      ],
      "metadata": {
        "id": "WgjvTHddPATd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "class NaiveBayesClassifier():\n",
        "    def __init__(self, *, a=2, b=2, split=90, N=100) -> None:\n",
        "        self.a = a\n",
        "        self.b = b\n",
        "        self.split = split\n",
        "        self.N = int(N)\n",
        "\n",
        "    def _map_pi_theta(self, X, y):\n",
        "        a = self.a\n",
        "        b = self.b\n",
        "        N, vocab_size = X.shape[0], X.shape[1]\n",
        "        pi = 0\n",
        "        theta = np.zeros([vocab_size, 3])\n",
        "\n",
        "        X_pizza = X[y == \"Pizza\"]\n",
        "        X_sushi = X[y == \"Sushi\"]\n",
        "        X_shawarma = X[y == \"Shawarma\"]\n",
        "\n",
        "        N_pizza = X_pizza.shape[0]\n",
        "        N_sushi = X_sushi.shape[0]\n",
        "        N_shawarma = X_shawarma.shape[0]\n",
        "\n",
        "        theta[:, 0] = (np.matmul(np.transpose(X_pizza), np.ones(N_pizza)) + a - 1) / (N_pizza + a + b - 2)\n",
        "        theta[:, 1] = (np.matmul(np.transpose(X_sushi), np.ones(N_sushi)) + a - 1) / (N_sushi + a + b - 2)\n",
        "        theta[:, 2] = (np.matmul(np.transpose(X_shawarma), np.ones(N_shawarma)) + a - 1) / (N_shawarma + a + b - 2)\n",
        "\n",
        "        pi = [N_pizza/N, N_sushi/N, N_shawarma/N]\n",
        "\n",
        "        return pi, theta\n",
        "\n",
        "    def _training_subset(self, X, y):\n",
        "        percent_split = self.split\n",
        "        X_random = np.array(X.copy())\n",
        "        y_random = np.array(y.copy())\n",
        "\n",
        "        p = np.random.permutation(len(y_random))\n",
        "        X_random, y_random = X_random[p], y_random[p]\n",
        "\n",
        "        slice1 = int(np.floor(percent_split * len(y_random) / 100))\n",
        "        return X_random[:slice1], y_random[:slice1]\n",
        "\n",
        "    def _single_prediction(self, X, pi, theta):\n",
        "      results = []\n",
        "\n",
        "      # Use log-probabilities instead of exponentiating\n",
        "      log_pi = np.log(pi)\n",
        "\n",
        "      log_pizza = np.matmul(X, np.log(theta[:, 0])) + np.matmul(1 - X, np.log(1 - theta[:, 0])) + log_pi[0]\n",
        "      results.append(log_pizza)\n",
        "\n",
        "      log_sushi = np.matmul(X, np.log(theta[:, 1])) + np.matmul(1 - X, np.log(1 - theta[:, 1])) + log_pi[1]\n",
        "      results.append(log_sushi)\n",
        "\n",
        "      log_shawarma = np.matmul(X, np.log(theta[:, 2])) + np.matmul(1 - X, np.log(1 - theta[:, 2])) + log_pi[2]\n",
        "      results.append(log_shawarma)\n",
        "\n",
        "      # Compare log scores directly\n",
        "      y = np.argmax(results, axis=0)\n",
        "      return y\n",
        "\n",
        "    def fit(self, X, y, sample_weight=None):\n",
        "        pi_map = []\n",
        "        theta_map = []\n",
        "        N = self.N\n",
        "        for i in range(N):\n",
        "            X_batch, y_batch = self._training_subset(X, y)\n",
        "            pi_map_temp, theta_map_temp = self._map_pi_theta(X_batch, y_batch)\n",
        "            pi_map.append(pi_map_temp)\n",
        "            theta_map.append(theta_map_temp)\n",
        "\n",
        "        self.pi_map = np.mean(pi_map, axis=0)\n",
        "        self.theta_map = np.mean(theta_map, axis=0)\n",
        "        self.theta_map = np.clip(self.theta_map, 1e-9, 1 - 1e-9)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        N = self.N\n",
        "        pi = self.pi_map\n",
        "        theta = self.theta_map\n",
        "        y_temp = self._single_prediction(X, pi, theta)\n",
        "\n",
        "        y_map = [\"Pizza\" if x==0 else \"Sushi\" if x==1 else \"Shawarma\" for x in y_temp]\n",
        "        return np.array(y_map)\n",
        "\n",
        "    def get_params(self, deep=False):\n",
        "        if deep:\n",
        "            params = {}\n",
        "            for parameter, value in self:\n",
        "                params[parameter] = value\n",
        "            return params\n",
        "        else:\n",
        "            return {\"a\": self.a, \"b\": self.b, \"N\": self.N, \"split\": self.split}\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n",
        "        return self\n",
        "\n",
        "    def save(self, filename: str):\n",
        "        try:\n",
        "            with open(filename, \"wb\") as f:\n",
        "                params = {\"pi\": self.pi_map, \"theta\": self.theta_map}\n",
        "                pickle.dump(params, f)\n",
        "            print(f\"Success! Naive Bayes exported to {filename}.\")\n",
        "        except pickle.PicklingError:\n",
        "            print(\"Error: Naive Bayes could not be pickled. Double-check the types of data\")\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error: {e}\")\n",
        "\n",
        "    def load_pretrained(self, filename: str):\n",
        "        try:\n",
        "            with open(filename, \"rb\") as f:\n",
        "                params = pickle.load(f)\n",
        "                self.pi_map = params[\"pi\"]\n",
        "                self.theta_map = params[\"theta\"]\n",
        "            print(f\"Success! Pre-trained Naive Bayes loaded from {filename}.\")\n",
        "        except pickle.UnpicklingError:\n",
        "            print(f\"Error: either {filename} is not a valid pickle file or is corrupted.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error: {e}\")\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Return probability estimates for each class (needed for soft voting).\"\"\"\n",
        "        if not hasattr(self, 'pi_map') or not hasattr(self, 'theta_map'):\n",
        "            raise ValueError(\"Model must be trained before calling predict_proba.\")\n",
        "\n",
        "        log_pi = np.log(self.pi_map)\n",
        "        log_theta = np.log(self.theta_map)\n",
        "        log_1_minus_theta = np.log(1 - self.theta_map)\n",
        "\n",
        "        log_probs = np.zeros((X.shape[0], 3))  # 3 classes\n",
        "\n",
        "        for i in range(3):\n",
        "            log_prob_class = np.matmul(X, log_theta[:, i]) + np.matmul(1 - X, log_1_minus_theta[:, i]) + log_pi[i]\n",
        "            log_probs[:, i] = log_prob_class\n",
        "\n",
        "        # Convert log-probs to actual probabilities using softmax for stability\n",
        "        max_log_probs = np.max(log_probs, axis=1, keepdims=True)\n",
        "        exp_log_probs = np.exp(log_probs - max_log_probs)\n",
        "        probs = exp_log_probs / np.sum(exp_log_probs, axis=1, keepdims=True)\n",
        "\n",
        "        return {\n",
        "            \"Pizza\": probs[:, 0],\n",
        "            \"Sushi\": probs[:, 1],\n",
        "            \"Shawarma\": probs[:, 2]\n",
        "        }\n"
      ],
      "metadata": {
        "id": "urEfi3X5PH-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest**"
      ],
      "metadata": {
        "id": "VXdPi14oPRAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "from typing import Tuple, Any, Optional, Union, List\n",
        "from collections import Counter\n",
        "import random\n",
        "import math\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "class DecisionTreeClassifier:\n",
        "    \"\"\" My implementation of sklearn's Decision Tree Classifier.\n",
        "\n",
        "    Note: this assumes that ALL features are discrete i.e. categorical.\n",
        "\n",
        "    SOURCES:\n",
        "    - https://medium.com/@cristianleo120/master-decision-trees-and-building-them-from-scratch-in-python-af173dafb836\n",
        "    - https://www.kaggle.com/code/fareselmenshawii/decision-tree-from-scratch\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, min_samples_split: int = 10, in_forest: bool = False, random_state: Optional[int] = None):\n",
        "        \"\"\" Initialize the decision tree classifier.\n",
        "\n",
        "        - min_samples_split: the minimum number of samples required to split an internal node.\n",
        "          I left out max_depth because during initial testing, the best value for it was None.\n",
        "        - in_forest: a boolean value which is True iff this tree is part of a forest\n",
        "        - random_state: an integer for reproducibility\n",
        "        - tree: the decision tree, as a nested dictionary...who is OOP idk her\n",
        "        \"\"\"\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.in_forest = in_forest\n",
        "        self.tree = None\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
        "        \"\"\" Train the model by building the tree based on X & y.\n",
        "\n",
        "        - X: training data matrix\n",
        "        - y: associated labels\n",
        "        Please ensure that both X & y are numpy arrays and not pandas dataframes\n",
        "        or series.\n",
        "        Assumes that all features are categorical, so make sure floats are rounded\n",
        "        to ints.\n",
        "        \"\"\"\n",
        "        self.tree = self.build_tree(X, y)\n",
        "\n",
        "    def build_tree(self, X: np.ndarray, y: np.ndarray) -> dict:\n",
        "        \"\"\" Train the model and return a dictionary representing the decision tree.\n",
        "\n",
        "        - X: training data matrix\n",
        "        - y: associated labels\n",
        "\n",
        "        The dictionary can be either a leaf or a subtree node representing a split\n",
        "        e.g.,\n",
        "        - leaf node {'label': 'Shawarma'}\n",
        "        - subtree node: {'feature': 'Avengers', 'value': 1, 'left': [data points['Avengers'] == 1] ,\n",
        "          'right': [all other data points] }\n",
        "        \"\"\"\n",
        "        # base case 1: all samples have same label\n",
        "        if len(np.unique(y)) <= 1:\n",
        "            return {'label': np.unique(y)[0]}\n",
        "        # base case 2: min_samples_split reached\n",
        "        if len(y) < self.min_samples_split:\n",
        "            unique_vals, counts = np.unique(y, return_counts=True)\n",
        "            majority_label = unique_vals[np.argmax(counts)]\n",
        "            return {'label': majority_label}\n",
        "\n",
        "        # recursive case\n",
        "        best_split = self.find_best_split(X, y)\n",
        "        if best_split is None:\n",
        "            unique_vals, counts = np.unique(y, return_counts=True)\n",
        "            majority_label = unique_vals[np.argmax(counts)]\n",
        "            return {'label': majority_label}\n",
        "\n",
        "        feature, split_on, left_X, left_y, right_X, right_y = best_split\n",
        "        # build subtrees recursively\n",
        "        left_subtree = self.build_tree(left_X, left_y)\n",
        "        right_subtree = self.build_tree(right_X, right_y)\n",
        "\n",
        "        return {'feature': feature,\n",
        "                'value': split_on,\n",
        "                'left': left_subtree,\n",
        "                'right': right_subtree}\n",
        "\n",
        "    def find_best_split(self, X: np.ndarray, y: np.ndarray) -> Optional[Tuple[str, Union[int, float, str], np.ndarray, np.ndarray, np.ndarray, np.ndarray]]:\n",
        "        \"\"\" Use Gini impurity to find the find the best feature and value to split on using Gini impurity.\n",
        "\n",
        "        - X: training data matrix\n",
        "        - y: associated labels\n",
        "\n",
        "        Returns a tuple containing the best feature, value, and its split:\n",
        "        left_X, left_y, right_X, right_y, or None if no split is found.\n",
        "        \"\"\"\n",
        "        best_gini = float('inf')\n",
        "        best_split = None\n",
        "\n",
        "        # if tree is part of forest, randomly choose subset of features\n",
        "        # subset size is sqrt(n_features)\n",
        "        n_features = X.shape[1]\n",
        "        features = list(range(n_features))\n",
        "        if self.in_forest:\n",
        "            max_features = round(math.sqrt(n_features))\n",
        "            if self.random_state:\n",
        "                random_state = np.random.RandomState(self.random_state) # to avoid impacting global numpy state\n",
        "                features = random_state.choice(features, max_features, replace=False).tolist()\n",
        "            else:\n",
        "                features = np.random.choice(features, max_features, replace=False).tolist()\n",
        "\n",
        "        for feature in features:  # test each feature\n",
        "            col = X[:, feature]\n",
        "            vals = np.unique(col)\n",
        "            for val in vals:  # test each split of this feature: one side == value, rest == not value\n",
        "                mask_left = col == val  # masque for vectorrrrization\n",
        "                mask_right = ~mask_left\n",
        "                y_l = y[mask_left]\n",
        "                y_r = y[mask_right]\n",
        "                # i am paranoid that a split w min samples will escape to here\n",
        "                if len(y_l) >= self.min_samples_split and len(y_r) >= self.min_samples_split:\n",
        "                    # calc Gini impurity of split\n",
        "                    split_impurity = self.split_impurity(y_l, y_r)\n",
        "                    if split_impurity < best_gini:\n",
        "                        best_gini = split_impurity\n",
        "                        best_split = (feature, val, X[mask_left], y_l, X[mask_right], y_r)\n",
        "\n",
        "        return best_split\n",
        "\n",
        "    def gini(self, y: np.ndarray) -> float:\n",
        "        \"\"\" Return the Gini impurity of a series y. Gini impurity = how often a\n",
        "        random datapoint would be labelled incorrectly. \"\"\"\n",
        "        # Gini = 1 - sum(p^2)\n",
        "        if len(y) == 0:\n",
        "            return 0.0\n",
        "        unique_vals, class_counts = np.unique(y, return_counts=True)\n",
        "        probabilities = class_counts / len(y)\n",
        "        impurity = 1 - np.sum(probabilities ** 2)\n",
        "        return impurity\n",
        "\n",
        "    def split_impurity(self, left_y: np.ndarray, right_y: np.ndarray) -> float:\n",
        "        \"\"\" Return the Gini impurity for a split. Remember: a lower Gini score\n",
        "        means the split is better.\n",
        "\n",
        "        - left_y: labels for the left data points (== value) after the split.\n",
        "        - right_y: labels for the right data points (!= value) after the split.\n",
        "        \"\"\"\n",
        "        # gini impurity for a split = n_left / n_total * Gini_left + n_right / n_total * Gini_right\n",
        "        Gini_l = self.gini(left_y)\n",
        "        Gini_r = self.gini(right_y)\n",
        "        n_left = len(left_y)\n",
        "        n_right = len(right_y)\n",
        "        n_total = n_left + n_right\n",
        "\n",
        "        split_impurity = (n_left / n_total * Gini_l) + (n_right / n_total * Gini_r)\n",
        "        return split_impurity\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\" Return the predicted class for each data point in X.\n",
        "        Please ensure that X is a numpy array and not a pandas dataframe or series.\n",
        "        Assumes that all features are categorical, so make sure floats are rounded\n",
        "        to ints.\n",
        "        \"\"\"\n",
        "        predictions = np.array([self.predict_single(row, self.tree) for row in X])\n",
        "        return predictions\n",
        "\n",
        "    def predict_single(self, row: np.ndarray, tree: dict) -> Any:\n",
        "        \"\"\" For ONE test point, go down the decision tree and return the\n",
        "        predicted class.\n",
        "\n",
        "        - row: a single test data point\n",
        "        - tree: the current decision tree/subtree/leaf we are at\n",
        "        \"\"\"\n",
        "        # base case: there is a leaf in self.tree\n",
        "        if 'label' in tree:\n",
        "            return tree['label']\n",
        "\n",
        "        # recursive case: there is a tree with branches in self.tree\n",
        "        if row[tree['feature']] == tree['value']:  # left subtree\n",
        "            return self.predict_single(row, tree['left'])\n",
        "        else:  # right subtree\n",
        "            return self.predict_single(row, tree['right'])\n",
        "\n",
        "\n",
        "class RandomForestClassifier():\n",
        "    \"\"\" My implementation of sklearn's RFC.\n",
        "    Bootstrapping and categorical features are assumed. Uses sqrt for max_features.\n",
        "\n",
        "    SOURCES:\n",
        "    - https://medium.com/@enozeren/building-a-random-forest-model-from-scratch-81583cbaa7a9\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators: int = 200, max_samples: Optional[float] = None, min_samples_split: int = 10, random_state: Optional[int] = None) -> None:\n",
        "        \"\"\" Initialize the RFC.\n",
        "\n",
        "        - n_estimators: the number of individual decision trees in this forest\n",
        "        - min_samples_split: the minimum number of samples needed to split an internal node\n",
        "        - max_samples: a float in the range (0.0, 1.0] which specifies the size of the bootstrap batch\n",
        "        used to train each individual tree, as a proportion of the original dataset size.\n",
        "        - random_state: for reproducibility\n",
        "        \"\"\"\n",
        "        if max_samples:\n",
        "            if (max_samples <= 0.0) or (max_samples > 1.0):\n",
        "                raise Exception(\"max_samples must be in the range (0.0, 1.0].\")\n",
        "\n",
        "        self.n_estimators = n_estimators\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_samples = max_samples\n",
        "        self.n_classes = None\n",
        "        self.trees = []\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def get_bootstrap_sample(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\" Get one bootstrap sample the size of self.max_samples * N (where N is the original\n",
        "        number of samples). Return this as a tuple of [X_sample, y_sample].\n",
        "\n",
        "        - X: training data matrix\n",
        "        - y: associated labels\n",
        "        \"\"\"\n",
        "        if self.random_state:  # don't touch global numpy state\n",
        "            random_state = np.random.RandomState(self.random_state)\n",
        "\n",
        "        n = X.shape[0]  # no. samples\n",
        "        if self.max_samples:  # max_samples is not None\n",
        "            bstrap_size = max(round(n * self.max_samples), 1)\n",
        "            if self.random_state:\n",
        "                indices = random_state.choice(n, size=bstrap_size, replace=True)  # sample w replacement\n",
        "            else:\n",
        "                indices = np.random.choice(n, size=bstrap_size, replace=True)  # sample w replacement\n",
        "        else:  # max_samples is None, sample full thing w/replacement (does not usually yield same dataset!)\n",
        "            if self.random_state:\n",
        "                indices = random_state.choice(n, size=n, replace=True)\n",
        "            else:\n",
        "                indices = np.random.choice(n, size=n, replace=True)\n",
        "\n",
        "        X_boot = X[indices]\n",
        "        y_boot = y[indices]\n",
        "        return X_boot, y_boot\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
        "        \"\"\" Fit the RFC to the training data.\n",
        "\n",
        "        - X: training data matrix\n",
        "        - y: associated labels\n",
        "\n",
        "        Please ensure that both X and y are numpy arrays and not pandas dataframes\n",
        "        or series.\n",
        "        Assumes that all features are categorical, so make sure floats are rounded\n",
        "        to ints.\n",
        "        \"\"\"\n",
        "        self.n_classes = len(np.unique(y))\n",
        "        for i in range(self.n_estimators):\n",
        "            X_boot, y_boot = self.get_bootstrap_sample(X, y)\n",
        "            tree = DecisionTreeClassifier(min_samples_split=self.min_samples_split, in_forest=True, random_state=self.random_state)\n",
        "            tree.fit(X_boot, y_boot)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\" Predict class labels for a set of samples.\n",
        "\n",
        "        Please ensure that X is a numpy array and not a pandas dataframe or series.\n",
        "        Assumes that all features are categorical, so make sure floats are rounded\n",
        "        to ints.\n",
        "        \"\"\"\n",
        "        tree_predictions = np.empty((self.n_estimators, X.shape[0]), dtype=object)\n",
        "\n",
        "        # get predictions from each tree\n",
        "        for i, tree in enumerate(self.trees):\n",
        "            tree_predictions[i, :] = tree.predict(X)\n",
        "\n",
        "        # take the mode (most common value) across trees for each sample\n",
        "        majority_votes = []\n",
        "        for i in range(X.shape[0]):\n",
        "            sample_votes = tree_predictions[:, i]\n",
        "            unique_values, counts = np.unique(sample_votes, return_counts=True)\n",
        "            majority_vote = unique_values[np.argmax(counts)]\n",
        "            majority_votes.append(majority_vote)\n",
        "\n",
        "        return np.array(majority_votes)\n",
        "\n",
        "    def save(self, filename: str):\n",
        "        \"\"\" Save the RFC model to a pickle file with filename so it can be used\n",
        "        out-of-the-box.\n",
        "        filename must be a pickle (.pkl) file, e.g. \"RFC_pretrained.pkl\".\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(filename, \"wb\") as f:\n",
        "                pickle.dump(self.trees, f)\n",
        "            print(f\"Success! RFC exported to {filename}.\")\n",
        "        except pickle.PicklingError:\n",
        "            print(\"Error: RFC could not be pickled. Double-check the types of data in the tree :(\")\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error: {e}\")\n",
        "\n",
        "    def load_pretrained(self, filename: str):\n",
        "        \"\"\" Load a pretrained RFC from filename.\n",
        "        filename must be a pickle (.pkl) file which contains a list of dictionaries,\n",
        "        where each dict represents one decision tree in the forest. \"\"\"\n",
        "        try:\n",
        "            with open(filename, \"rb\") as f:\n",
        "                self.trees = pickle.load(f)\n",
        "            print(f\"Success! Pre-trained RFC loaded from {filename}.\")\n",
        "        except pickle.UnpicklingError:\n",
        "            print(f\"Error: either {filename} is not a valid pickle file or is corrupted.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error: {e}\")\n",
        "\n",
        "    def predict_proba(self, X: np.ndarray) -> dict:\n",
        "        \"\"\"\n",
        "        Returns predicted probabilities as a dictionary: {class_label: probability_vector}\n",
        "        Each vector contains the estimated probability of that class for each input row.\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        class_labels = [\"Pizza\", \"Sushi\", \"Shawarma\"]\n",
        "        class_index = {label: i for i, label in enumerate(class_labels)}\n",
        "        votes = np.zeros((n_samples, len(class_labels)))\n",
        "\n",
        "        for tree in self.trees:\n",
        "            preds = tree.predict(X)\n",
        "            for i, pred in enumerate(preds):\n",
        "                votes[i, class_index[pred]] += 1\n",
        "\n",
        "        probs = votes / len(self.trees)\n",
        "        return {label: probs[:, idx] for idx, label in enumerate(class_labels)}\n"
      ],
      "metadata": {
        "id": "alE5CTmhPmVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**"
      ],
      "metadata": {
        "id": "D8Ata05iPpCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "class CustomLogisticRegression:\n",
        "    def __init__(self, lr=0.01, epochs=1000, reg_strength=0.0):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.reg_strength = reg_strength\n",
        "        self.weights = None\n",
        "        self.bias = 0.0\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        z = np.clip(z, -500, 500)  # prevent overflow\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.array(X, dtype=np.float64)\n",
        "        y = np.array(y, dtype=np.float64)\n",
        "\n",
        "        # Normalize X\n",
        "        self.X_min = X.min(axis=0)\n",
        "        self.X_max = X.max(axis=0)\n",
        "        X = (X - self.X_min) / (self.X_max - self.X_min + 1e-8)\n",
        "\n",
        "        m, n = X.shape\n",
        "        self.weights = np.zeros(n, dtype=np.float64)\n",
        "        self.bias = 0.0\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            linear = np.dot(X, self.weights) + self.bias\n",
        "            predictions = self.sigmoid(linear)\n",
        "            error = predictions - y\n",
        "            dw = (np.dot(X.T, error) + self.reg_strength * self.weights) / m\n",
        "            db = np.sum(error) / m\n",
        "\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "            if epoch % 100 == 0 or epoch == self.epochs - 1:\n",
        "                loss = -np.mean(y * np.log(predictions + 1e-9) + (1 - y) * np.log(1 - predictions + 1e-9))\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X = np.array(X, dtype=np.float64)\n",
        "        X = (X - self.X_min) / (self.X_max - self.X_min + 1e-8)\n",
        "        return self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
        "\n",
        "    def save(self, filename):\n",
        "        with open(filename, \"wb\") as f:\n",
        "            pickle.dump(self.classifiers, f)\n",
        "\n",
        "    def load_pretrained(self, filename):\n",
        "        with open(filename, \"rb\") as f:\n",
        "            self.classifiers = pickle.load(f)\n",
        "            self.classes = np.array(list(self.classifiers.keys()))\n",
        "\n",
        "\n",
        "class CustomMulticlassLogisticRegression:\n",
        "    def __init__(self, lr=0.01, epochs=1000, reg_strength=0.0):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.reg_strength = reg_strength\n",
        "        self.classifiers = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        for cls in self.classes:\n",
        "            y_binary = (y == cls).astype(int)\n",
        "            model = CustomLogisticRegression(\n",
        "                lr=self.lr, epochs=self.epochs, reg_strength=self.reg_strength\n",
        "            )\n",
        "            model.fit(X, y_binary)\n",
        "            self.classifiers[cls] = model\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return {cls: model.predict_proba(X) for cls, model in self.classifiers.items()}\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.predict_proba(X)\n",
        "        probs_matrix = np.column_stack([probs[cls] for cls in self.classes])\n",
        "        return self.classes[np.argmax(probs_matrix, axis=1)]\n",
        "\n",
        "    def save(self, filename):\n",
        "        with open(filename, \"wb\") as f:\n",
        "            pickle.dump(self.classifiers, f)\n",
        "\n",
        "    def load_pretrained(self, filename):\n",
        "        with open(filename, \"rb\") as f:\n",
        "            self.classifiers = pickle.load(f)\n",
        "            self.classes = np.array(list(self.classifiers.keys()))\n",
        "\n"
      ],
      "metadata": {
        "id": "3TbFQvdQPzZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the Models**"
      ],
      "metadata": {
        "id": "1Q8K0Y2MP1Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Save column names used in training after preprocessing\n",
        "training_feature_names = list(X.columns)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/final_feature_names.json\", \"w\") as f:\n",
        "    json.dump(training_feature_names, f)\n",
        "\n",
        "# Train and Save All 3 Models\n",
        "# Grid Search Values\n",
        "# a_values = [0.5, 1, 2, 3, 5]\n",
        "# b_values = [0.5, 1, 2, 3, 5]\n",
        "# N_values = [10, 25, 50, 75, 100, 150]\n",
        "# split_values = [70, 80, 85, 90, 95]\n",
        "\n",
        "nb_model = NaiveBayesClassifier(a=2, b=2, N=50, split=90)\n",
        "nb_model.fit(X_train, y_train)\n",
        "nb_preds = nb_model.predict(X_test)\n",
        "print(\"\\nNaive Bayes Accuracy:\", round(accuracy_score(y_test, nb_preds), 4))\n",
        "print(\"Classification Report (Naive Bayes):\")\n",
        "print(classification_report(y_test, nb_preds))\n",
        "\n",
        "nb_model_data = {\n",
        "    \"pi\": nb_model.pi_map.tolist(),      # convert numpy array to list (optional for readability)\n",
        "    \"theta\": nb_model.theta_map.tolist()\n",
        "}\n",
        "\n",
        "with open(\"final_nb_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(nb_model_data, f)\n",
        "\n",
        "# nb_model.save(\"/content/drive/MyDrive/final_nb_model.pkl\")\n",
        "\n",
        "# Grid Search Values\n",
        "# lr_list = [0.01, 0.05, 0.1, 0.2]\n",
        "# epochs_list = [1000, 2000, 3000, 4000, 5000]\n",
        "# reg_strength_list = [0.01, 0.1, 0.5, 1.0]\n",
        "\n",
        "lr_model = CustomMulticlassLogisticRegression(lr=0.1, epochs=4000, reg_strength=0.1)\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_preds = lr_model.predict(X_test)\n",
        "print(\"\\nLogistic Regression Accuracy:\", round(accuracy_score(y_test, lr_preds), 4))\n",
        "print(\"Classification Report (Logistic Regression):\")\n",
        "print(classification_report(y_test, lr_preds))\n",
        "# Manually extract each binary classifier's parameters\n",
        "lr_model_data = {}\n",
        "for cls, model in lr_model.classifiers.items():\n",
        "    lr_model_data[cls] = {\n",
        "        \"weights\": model.weights.tolist(),\n",
        "        \"bias\": model.bias,\n",
        "        \"X_min\": model.X_min.tolist(),\n",
        "        \"X_max\": model.X_max.tolist()\n",
        "    }\n",
        "\n",
        "# Save all classifiers into one dict\n",
        "with open(\"/content/drive/MyDrive/final_lr_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(lr_model_data, f)\n",
        "\n",
        "# Grid search\n",
        "# n_estimators_list = [100, 200, 300, 500, 750, 1000]\n",
        "# min_samples_split_list = [2, 4, 5, 8, 10, 15]\n",
        "# max_samples_list = [0.4, 0.5, 0.6, 0.8, 1.0]\n",
        "\n",
        "# best_acc = 0\n",
        "# best_params = None\n",
        "# best_model = None\n",
        "\n",
        "# param_grid = list(itertools.product(n_estimators_list, min_samples_split_list, max_samples_list))\n",
        "# print(f\"Trying {len(param_grid)} combinations...\")\n",
        "\n",
        "# for n_estimators, min_samples_split, max_samples in param_grid:\n",
        "#     print(f\"\\nTrying: n_estimators={n_estimators}, min_samples_split={min_samples_split}, max_samples={max_samples}\")\n",
        "\n",
        "#     model = RandomForestClassifier(\n",
        "#         n_estimators=n_estimators,\n",
        "#         min_samples_split=min_samples_split,\n",
        "#         max_samples=max_samples\n",
        "#     )\n",
        "#     model.fit(X_train, y_train)\n",
        "#     preds = model.predict(X_test)\n",
        "#     acc = accuracy_score(y_test, preds)\n",
        "\n",
        "#     print(f\"   â Accuracy: {round(acc, 4)}\")\n",
        "\n",
        "#     if acc > best_acc:\n",
        "#         best_acc = acc\n",
        "#         best_params = (n_estimators, min_samples_split, max_samples)\n",
        "#         best_model = model\n",
        "\n",
        "# print(\"\\nBest Hyperparameters:\")\n",
        "# print(f\"n_estimators={best_params[0]}, min_samples_split={best_params[1]}, max_samples={best_params[2]}\")\n",
        "# print(f\"Best Accuracy: {round(best_acc, 4)}\")\n",
        "\n",
        "# best_model.save(\"/content/drive/MyDrive/final_rf_model.pkl\")\n",
        "\n",
        "# print(classification_report(y_test, best_model.predict(X_test)))\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, min_samples_split=2, max_samples=1.0)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_preds = rf_model.predict(X_test)\n",
        "print(\"\\nRandom Forest Accuracy:\", round(accuracy_score(y_test, rf_preds), 4))\n",
        "print(\"Classification Report (Random Forest):\")\n",
        "print(classification_report(y_test, rf_preds))\n",
        "# Extract all trees as dictionaries\n",
        "rf_model_data = [tree.tree for tree in rf_model.trees]\n",
        "\n",
        "# Save as a plain list of dicts (safe for loading)\n",
        "with open(\"/content/drive/MyDrive/final_rf_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(rf_model_data, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cqXKs5PRecd",
        "outputId": "10a741bb-47b3-4d79-af31-fc03adaeecc3"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Naive Bayes Accuracy: 0.8747\n",
            "Classification Report (Naive Bayes):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Pizza       0.83      0.95      0.88       165\n",
            "    Shawarma       0.87      0.90      0.88       165\n",
            "       Sushi       0.95      0.78      0.86       165\n",
            "\n",
            "    accuracy                           0.87       495\n",
            "   macro avg       0.88      0.87      0.87       495\n",
            "weighted avg       0.88      0.87      0.87       495\n",
            "\n",
            "\n",
            "Logistic Regression Accuracy: 0.8889\n",
            "Classification Report (Logistic Regression):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Pizza       0.87      0.94      0.90       165\n",
            "    Shawarma       0.89      0.88      0.88       165\n",
            "       Sushi       0.91      0.85      0.88       165\n",
            "\n",
            "    accuracy                           0.89       495\n",
            "   macro avg       0.89      0.89      0.89       495\n",
            "weighted avg       0.89      0.89      0.89       495\n",
            "\n",
            "\n",
            "Random Forest Accuracy: 0.8545\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Pizza       0.79      0.95      0.86       165\n",
            "    Shawarma       0.87      0.90      0.88       165\n",
            "       Sushi       0.94      0.72      0.81       165\n",
            "\n",
            "    accuracy                           0.85       495\n",
            "   macro avg       0.87      0.85      0.85       495\n",
            "weighted avg       0.87      0.85      0.85       495\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save Complete Pkl Files for Testing**"
      ],
      "metadata": {
        "id": "Ci_RDRNbfXgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_model = NaiveBayesClassifier(a=2, b=2, N=50, split=90)\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/final_nb_model_full.pkl\", \"wb\") as f:\n",
        "    pickle.dump(nb_model, f)\n",
        "\n",
        "lr_model = CustomMulticlassLogisticRegression(lr=0.1, epochs=4000, reg_strength=0.1)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/final_lr_model_full.pkl\", \"wb\") as f:\n",
        "    pickle.dump(lr_model, f)\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, min_samples_split=2, max_samples=1.0)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/final_rf_model_full.pkl\", \"wb\") as f:\n",
        "    pickle.dump(rf_model, f)\n"
      ],
      "metadata": {
        "id": "6Qj0-vytfcC3"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Voting Classifier **"
      ],
      "metadata": {
        "id": "-T1HhLNdTnAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# --- Load Full Pretrained Models ---\n",
        "with open(\"/content/drive/MyDrive/final_nb_model_full.pkl\", \"rb\") as f:\n",
        "    nb = pickle.load(f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/final_lr_model_full.pkl\", \"rb\") as f:\n",
        "    lr = pickle.load(f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/final_rf_model_full.pkl\", \"rb\") as f:\n",
        "    rf = pickle.load(f)\n",
        "\n",
        "# --- Predict Probabilities ---\n",
        "nb_probs = nb.predict_proba(X_test)\n",
        "lr_probs = lr.predict_proba(X_test)\n",
        "rf_probs = rf.predict_proba(X_test)\n",
        "\n",
        "# --- Soft Voting Ensemble ---\n",
        "class_labels = [\"Pizza\", \"Sushi\", \"Shawarma\"]\n",
        "weights = {\"nb\": 0.1, \"lr\": 0.8, \"rf\": 0.1}\n",
        "\n",
        "avg_probs = {\n",
        "    label: weights[\"nb\"] * nb_probs[label] +\n",
        "           weights[\"lr\"] * lr_probs[label] +\n",
        "           weights[\"rf\"] * rf_probs[label]\n",
        "    for label in class_labels\n",
        "}\n",
        "\n",
        "final_preds = []\n",
        "for i in range(X_test.shape[0]):\n",
        "    label_scores = {label: avg_probs[label][i] for label in class_labels}\n",
        "    final_preds.append(max(label_scores, key=label_scores.get))\n",
        "\n",
        "# --- Evaluate ---\n",
        "final_preds = np.array(final_preds)\n",
        "print(\"\\nWeighted Voting Accuracy:\", round(accuracy_score(y_test, final_preds), 4))\n",
        "print(\"Classification Report (Voting Ensemble):\")\n",
        "print(classification_report(y_test, final_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5f-TOI8SHAg",
        "outputId": "6c9c5dc3-46b5-4aa1-8af7-38211c77b6d6"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Weighted Voting Accuracy: 0.8949\n",
            "Classification Report (Voting Ensemble):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Pizza       0.87      0.94      0.90       165\n",
            "    Shawarma       0.90      0.89      0.89       165\n",
            "       Sushi       0.93      0.85      0.89       165\n",
            "\n",
            "    accuracy                           0.89       495\n",
            "   macro avg       0.90      0.89      0.89       495\n",
            "weighted avg       0.90      0.89      0.89       495\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pred.py file**"
      ],
      "metadata": {
        "id": "lYv7cQHPNLBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "import json\n",
        "\n",
        "# --- Preprocessing Parameters ---\n",
        "numerical_columns = [\n",
        "    \"Q1: From a scale 1 to 5, how complex is it to make this food? (Where 1 is the most simple, and 5 is the most complex)\",\n",
        "    \"Q2: How many ingredients would you expect this food item to contain?\",\n",
        "    \"Q4: How much would you expect to pay for one serving of this food item?\"\n",
        "]\n",
        "\n",
        "text_cols = [\n",
        "    \"Q3: In what setting would you expect this food to be served? Please check all that apply\",\n",
        "    \"Q5: What movie do you think of when thinking of this food item?\",\n",
        "    \"Q6: What drink would you pair with this food item?\",\n",
        "    \"Q7: When you think about this food item, who does it remind you of?\"\n",
        "]\n",
        "\n",
        "q8_col = \"Q8: How much hot sauce would you add to this food item?\"\n",
        "hot_sauce_map = {\n",
        "    \"A little (mild)\": \"Mild\",\n",
        "    \"A moderate amount (medium)\": \"Medium\",\n",
        "    \"A lot (hot)\": \"Hot\",\n",
        "    \"I will have some of this food item with my hot sauce\": \"Medium\"\n",
        "}\n",
        "\n",
        "# --- Preprocessing ---\n",
        "def extract_numeric(value):\n",
        "    if pd.isnull(value):\n",
        "        return None\n",
        "    numbers = re.findall(r\"\\d+\\.?\\d*\", str(value))\n",
        "    return np.mean([float(n) for n in numbers]) if numbers else None\n",
        "\n",
        "def simple_bow(df, column_name, prefix):\n",
        "    vocab = set()\n",
        "    tokenized = []\n",
        "    for text in df[column_name]:\n",
        "        tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "        tokenized.append(tokens)\n",
        "        vocab.update(tokens)\n",
        "    vocab = sorted(vocab)\n",
        "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
        "    bow_matrix = np.zeros((len(df), len(vocab)), dtype=int)\n",
        "    for i, tokens in enumerate(tokenized):\n",
        "        for token in tokens:\n",
        "            if token in vocab_index:\n",
        "                bow_matrix[i, vocab_index[token]] = 1\n",
        "    return pd.DataFrame(bow_matrix, columns=[f\"{prefix}_{word}\" for word in vocab])\n",
        "\n",
        "def preprocess(df):\n",
        "    df = df.copy()\n",
        "    if \"Label\" in df.columns:\n",
        "        df = df.drop(columns=[\"Label\"])\n",
        "\n",
        "    for col in numerical_columns:\n",
        "        df[col] = df[col].apply(extract_numeric)\n",
        "        df[col] = df[col].fillna(df[col].mean())\n",
        "\n",
        "    for col in text_cols:\n",
        "        df[col] = df[col].fillna(\"none\").str.lower().str.strip()\n",
        "\n",
        "    bow_frames = []\n",
        "    for i, col in enumerate(text_cols):\n",
        "        bow_df = simple_bow(df, col, f\"Q{i+3}\")\n",
        "        bow_frames.append(bow_df)\n",
        "\n",
        "    df = pd.concat([df] + bow_frames, axis=1)\n",
        "    df.drop(columns=text_cols, inplace=True)\n",
        "\n",
        "    df[\"Q8_cleaned\"] = df[q8_col].map(hot_sauce_map).fillna(\"None\")\n",
        "    for category in df[\"Q8_cleaned\"].unique():\n",
        "        df[f\"Q8_cleaned_{category}\"] = (df[\"Q8_cleaned\"] == category).astype(int)\n",
        "    df.drop(columns=[q8_col, \"Q8_cleaned\"], inplace=True)\n",
        "\n",
        "    df.fillna(0, inplace=True)\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype in [\"float64\", \"bool\"]:\n",
        "            df[col] = df[col].astype(int)\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/final_feature_names.json\", \"r\") as f:\n",
        "        expected_columns = json.load(f)\n",
        "\n",
        "    missing_cols = [col for col in expected_columns if col not in df.columns]\n",
        "    df_missing = pd.DataFrame(0, index=df.index, columns=missing_cols)\n",
        "    df = pd.concat([df, df_missing], axis=1)\n",
        "    df = df[expected_columns].copy()\n",
        "    return df.astype(np.float64).values\n",
        "\n",
        "# --- Prediction ---\n",
        "def predict_all(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    X = preprocess(df)\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/final_nb_model.pkl\", \"rb\") as f:\n",
        "        nb_params = pickle.load(f)\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/final_lr_model.pkl\", \"rb\") as f:\n",
        "        lr_params = pickle.load(f)\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/final_rf_model.pkl\", \"rb\") as f:\n",
        "        rf_trees = pickle.load(f)\n",
        "\n",
        "    def nb_predict_proba(X, pi, theta):\n",
        "        log_pi = np.log(pi)\n",
        "        log_theta = np.log(theta)\n",
        "        log_1_theta = np.log(1 - theta)\n",
        "        log_probs = np.zeros((X.shape[0], 3))\n",
        "        for i in range(3):\n",
        "            log_probs[:, i] = X @ log_theta[:, i] + (1 - X) @ log_1_theta[:, i] + log_pi[i]\n",
        "        exp = np.exp(log_probs - np.max(log_probs, axis=1, keepdims=True))\n",
        "        probs = exp / np.sum(exp, axis=1, keepdims=True)\n",
        "        return {\n",
        "            \"Pizza\": probs[:, 0],\n",
        "            \"Sushi\": probs[:, 1],\n",
        "            \"Shawarma\": probs[:, 2]\n",
        "        }\n",
        "\n",
        "    nb_probs = nb_predict_proba(X, np.array(nb_params[\"pi\"]), np.array(nb_params[\"theta\"]))\n",
        "\n",
        "    def lr_predict_proba(X, classifiers):\n",
        "        probs = {}\n",
        "        class_order = list(classifiers.keys())\n",
        "        prob_matrix = np.zeros((X.shape[0], len(class_order)))\n",
        "        for i, cls in enumerate(class_order):\n",
        "            model = classifiers[cls]\n",
        "            X_scaled = (X - np.array(model[\"X_min\"])) / (np.array(model[\"X_max\"]) - np.array(model[\"X_min\"]) + 1e-8)\n",
        "            z = np.clip(X_scaled @ np.array(model[\"weights\"]) + model[\"bias\"], -500, 500)\n",
        "            prob_matrix[:, i] = 1 / (1 + np.exp(-z))\n",
        "            probs[cls] = prob_matrix[:, i]\n",
        "        return probs\n",
        "\n",
        "    lr_probs = lr_predict_proba(X, lr_params)\n",
        "\n",
        "    def predict_tree(tree, x):\n",
        "        while isinstance(tree, dict) and 'label' not in tree:\n",
        "            feature = tree[\"feature\"]\n",
        "            value = tree[\"value\"]\n",
        "            if x[feature] == value:\n",
        "                tree = tree[\"left\"]\n",
        "            else:\n",
        "                tree = tree[\"right\"]\n",
        "        return tree[\"label\"]\n",
        "\n",
        "    def rf_predict_proba(X, trees):\n",
        "        class_labels = [\"Pizza\", \"Sushi\", \"Shawarma\"]\n",
        "        label_to_idx = {l: i for i, l in enumerate(class_labels)}\n",
        "        votes = np.zeros((X.shape[0], 3))\n",
        "        for tree in trees:\n",
        "            preds = [predict_tree(tree, x) for x in X]\n",
        "            for i, p in enumerate(preds):\n",
        "                votes[i][label_to_idx[p]] += 1\n",
        "        probs = votes / len(trees)\n",
        "        return {\n",
        "            \"Pizza\": probs[:, 0],\n",
        "            \"Sushi\": probs[:, 1],\n",
        "            \"Shawarma\": probs[:, 2]\n",
        "        }\n",
        "\n",
        "    rf_probs = rf_predict_proba(X, rf_trees)\n",
        "\n",
        "    # --- Soft Voting ---\n",
        "    labels = [\"Pizza\", \"Sushi\", \"Shawarma\"]\n",
        "    weights = {\"nb\": 0.1, \"lr\": 0.8, \"rf\": 0.1}\n",
        "    final_probs = {}\n",
        "    for label in labels:\n",
        "        final_probs[label] = (\n",
        "            weights[\"nb\"] * nb_probs[label] +\n",
        "            weights[\"lr\"] * lr_probs[label] +\n",
        "            weights[\"rf\"] * rf_probs[label]\n",
        "        )\n",
        "\n",
        "    predictions = []\n",
        "    for i in range(X.shape[0]):\n",
        "        label_scores = {label: final_probs[label][i] for label in labels}\n",
        "        best = max(label_scores, key=label_scores.get)\n",
        "        predictions.append(best)\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "SgvcmadZF6Uq"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Pred.py**"
      ],
      "metadata": {
        "id": "WMuWo_snNpmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Load full original data (with raw Q1âQ8 + Label) ---\n",
        "df_full = pd.read_csv(\"/content/drive/MyDrive/cleaned_data_combined_modified.csv\")\n",
        "X_full = df_full.drop(columns=[\"Label\"])\n",
        "y_full = df_full[\"Label\"]\n",
        "\n",
        "# --- Custom stratified split to get test indices only ---\n",
        "def get_test_indices(X, y, test_size=0.3, random_state=42):\n",
        "    np.random.seed(random_state)\n",
        "    X = X.reset_index(drop=True)\n",
        "    y = y.reset_index(drop=True)\n",
        "    test_idx = []\n",
        "\n",
        "    for label in y.unique():\n",
        "        idx = y[y == label].index.tolist()\n",
        "        np.random.shuffle(idx)\n",
        "        split = int(len(idx) * (1 - test_size))\n",
        "        test_idx += idx[split:]\n",
        "\n",
        "    return test_idx\n",
        "\n",
        "# --- Extract raw test set from original data ---\n",
        "test_indices = get_test_indices(X_full, y_full)\n",
        "df_test = df_full.iloc[test_indices]\n",
        "\n",
        "# --- Save test split for evaluation and submission/testing ---\n",
        "test_csv_path = \"/content/drive/MyDrive/test_split_raw.csv\"\n",
        "df_test.to_csv(test_csv_path, index=False)\n",
        "print(f\"Saved test split with {len(df_test)} rows to:\", test_csv_path)\n",
        "\n",
        "# --- Run prediction on the raw test split ---\n",
        "true_labels = df_test[\"Label\"].values\n",
        "preds = predict_all(test_csv_path)\n",
        "\n",
        "# --- Evaluate accuracy ---\n",
        "accuracy = np.mean(np.array(preds) == true_labels)\n",
        "print(f\"\\nAccuracy on test split: {accuracy:.4f}\")\n",
        "\n",
        "# Optional: Confusion Matrix\n",
        "def print_confusion_matrix(y_true, y_pred, labels=[\"Pizza\", \"Sushi\", \"Shawarma\"]):\n",
        "    matrix = np.zeros((len(labels), len(labels)), dtype=int)\n",
        "    label_to_idx = {label: i for i, label in enumerate(labels)}\n",
        "\n",
        "    for true, pred in zip(y_true, y_pred):\n",
        "        matrix[label_to_idx[true]][label_to_idx[pred]] += 1\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(pd.DataFrame(matrix, index=labels, columns=labels))\n",
        "\n",
        "print_confusion_matrix(true_labels, preds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6E5KBrbJm5P",
        "outputId": "b9d94ee4-7758-49b4-c2cf-54eb62fc7574"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved test split with 495 rows to: /content/drive/MyDrive/test_split_raw.csv\n",
            "\n",
            "Accuracy on test split: 0.8949\n",
            "\n",
            "Confusion Matrix:\n",
            "          Pizza  Sushi  Shawarma\n",
            "Pizza       155      4         6\n",
            "Sushi        13    141        11\n",
            "Shawarma     11      7       147\n"
          ]
        }
      ]
    }
  ]
}